{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"2025-11-27 \u2014 The Case of Multiple Repos Why your connected systems should live in a single repository. 2025-10-31 \u2014 Working on FastAPI-Restly A REST framework for FastAPI that gives you CRUD with minimal code.","title":"Home"},{"location":"boilerplate/","text":"On boilerplate code Boilerplate code might seem harmless, but it can subtly bload a codebase and make it seem a lot larger and more complicated than it really is. What do I mean with boilerplate code? Maybe I'm also talking about a related concept, which is \"organizational code\". All code related to not actually the business logic, but organizing the business logic. For example, if you split code into two files, then imports from one file to the other are organization code. A function definition and function call are also organizational code. Clearly, some organization code is necessary. And functions are a great idea. However it is important to remember there are downsides to functions. First of all, they are an indirection. Wherever your see a function call, you have to make the decision \"Am I going to lookup the function definition and read it?\". This complicates reading. Functions with very good names and clear goals will allow you to skip over them, which is a gain in readability if it hides details you know you don't care about. At other times, functions (and other objects) are used to semantically organize code. Model-View-Controller would be an example of that. Model classes go into models, view functions in views, and the glue to make it all work goes into controller. This is nice if you are new to a code base and are just looking at the file structure. But as soon as you start reading you find yourself switching from file to file. Very often in applications functions in the controllers are only called from one place: The view functions. So the functions are not there for reusability, but rather for code organization. Generally what I see is that the organization is per code pattern. So view functions all follow a pattern described by a web framework. And the controller functions all follow some application specific or no pattern.","title":"On boilerplate code"},{"location":"boilerplate/#on-boilerplate-code","text":"Boilerplate code might seem harmless, but it can subtly bload a codebase and make it seem a lot larger and more complicated than it really is. What do I mean with boilerplate code? Maybe I'm also talking about a related concept, which is \"organizational code\". All code related to not actually the business logic, but organizing the business logic. For example, if you split code into two files, then imports from one file to the other are organization code. A function definition and function call are also organizational code. Clearly, some organization code is necessary. And functions are a great idea. However it is important to remember there are downsides to functions. First of all, they are an indirection. Wherever your see a function call, you have to make the decision \"Am I going to lookup the function definition and read it?\". This complicates reading. Functions with very good names and clear goals will allow you to skip over them, which is a gain in readability if it hides details you know you don't care about. At other times, functions (and other objects) are used to semantically organize code. Model-View-Controller would be an example of that. Model classes go into models, view functions in views, and the glue to make it all work goes into controller. This is nice if you are new to a code base and are just looking at the file structure. But as soon as you start reading you find yourself switching from file to file. Very often in applications functions in the controllers are only called from one place: The view functions. So the functions are not there for reusability, but rather for code organization. Generally what I see is that the organization is per code pattern. So view functions all follow a pattern described by a web framework. And the controller functions all follow some application specific or no pattern.","title":"On boilerplate code"},{"location":"fastapi-restly/","text":"Working on FastAPI-Restly 2025-10-31 Or more correctly, not enough working on FastAPI Restly. Which is a REST framework, like the venerable DRF, but different. The nice people at ClearBlue Markets allowed me to open source this part of the work that I did there. It is such a pain to write CRUD in FastAPI. There are CRUD libraries out there, but when I was checking (almost two years ago now) I didn't like any of them. So I wrote my own. It is not actually the first time I did this. During my time at EclecticIQ, I wrote something similar for Flask, but flask-classy was doing part of the heavy lifting and writing a CRUD class from there was not so hard. Still, you want steady patterns when tieing it together with SQLAlchemy and I got to nice declarative point which made me really happy. import fastapi_restly as fr class ArticleView(fr.AsyncAlchemyView): prefix = \"/articles\" model = Article This code gives CRUD. It creates several pydantic schemas under the hood, as well as a FastAPI router and several FastAPI endpoints. All of which can be overridden and customized. I'm very happy with what I've got so far. But there is still plenty to do before I can publish it to PyPI. Primarily it needs a great deal of documentation. But I'd also like to add more tests; I really want it to be rock-solid and there are some features there that I don't fully trust yet. This thing is actually in production at two companies where I worked. So it works, and it works well. But I've been the one applying it, and I'm sure other developers will use it in ways I didn't think of.","title":"Working on FastAPI-Restly"},{"location":"fastapi-restly/#working-on-fastapi-restly","text":"2025-10-31 Or more correctly, not enough working on FastAPI Restly. Which is a REST framework, like the venerable DRF, but different. The nice people at ClearBlue Markets allowed me to open source this part of the work that I did there. It is such a pain to write CRUD in FastAPI. There are CRUD libraries out there, but when I was checking (almost two years ago now) I didn't like any of them. So I wrote my own. It is not actually the first time I did this. During my time at EclecticIQ, I wrote something similar for Flask, but flask-classy was doing part of the heavy lifting and writing a CRUD class from there was not so hard. Still, you want steady patterns when tieing it together with SQLAlchemy and I got to nice declarative point which made me really happy. import fastapi_restly as fr class ArticleView(fr.AsyncAlchemyView): prefix = \"/articles\" model = Article This code gives CRUD. It creates several pydantic schemas under the hood, as well as a FastAPI router and several FastAPI endpoints. All of which can be overridden and customized. I'm very happy with what I've got so far. But there is still plenty to do before I can publish it to PyPI. Primarily it needs a great deal of documentation. But I'd also like to add more tests; I really want it to be rock-solid and there are some features there that I don't fully trust yet. This thing is actually in production at two companies where I worked. So it works, and it works well. But I've been the one applying it, and I'm sure other developers will use it in ways I didn't think of.","title":"Working on FastAPI-Restly"},{"location":"the-case-of-multiple-repos/","text":"The Case of Multiple Repos 2025-11-27 Your whole project should be in a single git repo! Really, it does! Multiple repos make teams slow and systems error prone. If the frontend , backend , infrastructure , computation_service , data_pipelines , forecast_model should be working together they should be in a single repo. Too often have I seen this not being case, and the effects are more costly and insidious than you'd think at first. Let's take frontend and backend , now this might be less common these days because fullstack developers will generally not create two repositories (for reasons I will go into later). But how do you make sure your frontend and backend work correctly together if they are in separate repositories? You will be adding another layer of version control. Let's say a new feature is implemented in separate frontend and backend repos. How do you deploy it? You make sure the new backend is deployed before the new frontend, right? Next you'll be wanting to add tags with a release number so you remember which frontend commit relies on which backend commit. These release tags are the second layer of version control. One that you are managing manually. Versioned releases are great - if you are providing a library or a public API. But if this is an application , versions are really only useful for release notes and bug reports. The downsides and costs of second-layer versioning should not be underestimated. It goes further than the frustrated frontender that forgot to update the backend repository again. Crucial development processes become more complicated: How do you manage deploying the different subsystems? How do you run the complete system locally? How do you test end-to-end? How do you do code review over a complete feature? These issues will slow down your team; There is more to manage, you will find bugs later and will have a whole class of bugs that are not possible in a single repo. And there are other, less obvious problems. Separate repositories create knowledge silos and ownership silos, and lower overall collaboration. They increase the friction of cross-functional work. Developers don't naturally see merge requests for other parts of a system and will be less inclined to get involved. In a way, it is an example of Conway's Law: System architecture follows organizational structure. This is especially a risk in new teams or new companies. Developers don't know each other well yet. The team is not yet a real team. It may feel natural for each developer to start a git repository for their own subdomain. Another reason this might happen is because developers are unsure where to put something new. They might not feel enough ownership over the project root, or don't feel comfortable reaching out to discuss the placement of this new part. You don't have to go full monorepo, Google-style. Instead ideally you look at what parts are connected, and put as many of those connected parts in a single repository, until you actually reach other practical limits. I'm not sure what those limits are or where they lie, but they are certainly a lot further away than commonly assumed. Hit those limits first and then think about where a second layer of versioning will be worth the hassle. These days, frontend and backend is more likely to be done by fullstack developers. They will use a single repo for both, because they won't feel this friction in collaboration. But infrastructure engineers, data scientists, and data engineers are still likely to be working in their own repositories. Or each data model may have its own repository, with its own full set of project dependencies, Makefiles, and CI/CD. Yet another tiny little project that needs managing. If you find yourself dealing with integration errors, or versioning to prevent integration errors: Consider merging repositories.","title":"The Case of Multiple Repos"},{"location":"the-case-of-multiple-repos/#the-case-of-multiple-repos","text":"2025-11-27 Your whole project should be in a single git repo! Really, it does! Multiple repos make teams slow and systems error prone. If the frontend , backend , infrastructure , computation_service , data_pipelines , forecast_model should be working together they should be in a single repo. Too often have I seen this not being case, and the effects are more costly and insidious than you'd think at first. Let's take frontend and backend , now this might be less common these days because fullstack developers will generally not create two repositories (for reasons I will go into later). But how do you make sure your frontend and backend work correctly together if they are in separate repositories? You will be adding another layer of version control. Let's say a new feature is implemented in separate frontend and backend repos. How do you deploy it? You make sure the new backend is deployed before the new frontend, right? Next you'll be wanting to add tags with a release number so you remember which frontend commit relies on which backend commit. These release tags are the second layer of version control. One that you are managing manually. Versioned releases are great - if you are providing a library or a public API. But if this is an application , versions are really only useful for release notes and bug reports. The downsides and costs of second-layer versioning should not be underestimated. It goes further than the frustrated frontender that forgot to update the backend repository again. Crucial development processes become more complicated: How do you manage deploying the different subsystems? How do you run the complete system locally? How do you test end-to-end? How do you do code review over a complete feature? These issues will slow down your team; There is more to manage, you will find bugs later and will have a whole class of bugs that are not possible in a single repo. And there are other, less obvious problems. Separate repositories create knowledge silos and ownership silos, and lower overall collaboration. They increase the friction of cross-functional work. Developers don't naturally see merge requests for other parts of a system and will be less inclined to get involved. In a way, it is an example of Conway's Law: System architecture follows organizational structure. This is especially a risk in new teams or new companies. Developers don't know each other well yet. The team is not yet a real team. It may feel natural for each developer to start a git repository for their own subdomain. Another reason this might happen is because developers are unsure where to put something new. They might not feel enough ownership over the project root, or don't feel comfortable reaching out to discuss the placement of this new part. You don't have to go full monorepo, Google-style. Instead ideally you look at what parts are connected, and put as many of those connected parts in a single repository, until you actually reach other practical limits. I'm not sure what those limits are or where they lie, but they are certainly a lot further away than commonly assumed. Hit those limits first and then think about where a second layer of versioning will be worth the hassle. These days, frontend and backend is more likely to be done by fullstack developers. They will use a single repo for both, because they won't feel this friction in collaboration. But infrastructure engineers, data scientists, and data engineers are still likely to be working in their own repositories. Or each data model may have its own repository, with its own full set of project dependencies, Makefiles, and CI/CD. Yet another tiny little project that needs managing. If you find yourself dealing with integration errors, or versioning to prevent integration errors: Consider merging repositories.","title":"The Case of Multiple Repos"}]}